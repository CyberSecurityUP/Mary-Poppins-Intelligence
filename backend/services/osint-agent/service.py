"""
Mary Poppins — OSINT AI Agent Service
LLM-powered investigative assistant for OSINT operations.

Provides conversational AI guidance for:
- Analyzing OSINT findings and suggesting next steps
- Pattern identification across search results
- Investigation summarization and reporting
- Query suggestion based on current evidence

Uses the LLM Service for provider-agnostic model access.
"""

from __future__ import annotations

import logging
import time
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Optional

logger = logging.getLogger("mp.osint_agent")


# ---------------------------------------------------------------------------
# Data Classes
# ---------------------------------------------------------------------------

class AgentRole(str, Enum):
    USER = "user"
    ASSISTANT = "assistant"
    SYSTEM = "system"


@dataclass
class SuggestedQuery:
    """A search query suggested by the AI agent."""
    query_type: str  # email, username, domain, ip_address, phone, name
    query_value: str
    rationale: str
    priority: str = "medium"  # high, medium, low


@dataclass
class AgentMessage:
    """A single message in the agent conversation."""
    role: AgentRole
    content: str
    timestamp: datetime = field(default_factory=datetime.utcnow)
    suggested_queries: list[SuggestedQuery] = field(default_factory=list)
    metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class AgentContext:
    """Context provided to the agent for informed responses."""
    current_query_type: Optional[str] = None
    current_query_value: Optional[str] = None
    findings_summary: list[dict[str, Any]] = field(default_factory=list)
    case_id: Optional[str] = None
    case_title: Optional[str] = None
    investigation_id: Optional[str] = None


@dataclass
class InvestigationSummary:
    """Structured investigation summary generated by the agent."""
    title: str
    key_findings: list[str]
    risk_assessment: str
    recommended_actions: list[str]
    entities_of_interest: list[dict[str, str]]
    generated_at: datetime = field(default_factory=datetime.utcnow)


# ---------------------------------------------------------------------------
# System Prompts
# ---------------------------------------------------------------------------

OSINT_AGENT_SYSTEM_PROMPT = """You are an OSINT Investigation AI Agent for the Mary Poppins Digital Intelligence Platform.

Your role is to assist law enforcement analysts with open-source intelligence investigations.

CAPABILITIES:
- Analyze OSINT findings (breaches, username matches, domain data, IP intelligence)
- Suggest follow-up search queries based on current evidence
- Identify patterns and correlations across findings
- Provide investigative guidance and prioritization
- Summarize investigation progress

RULES:
1. NEVER fabricate data. Only reference findings that are actually provided.
2. Always cite which specific findings support your analysis.
3. Suggest actionable next steps with clear rationale.
4. Flag high-risk indicators (Tor usage, dark web mentions, known breaches).
5. Maintain professional, objective language appropriate for legal proceedings.
6. If asked about content you cannot analyze, clearly state your limitations.
7. Protect privacy — only discuss entities within the investigation scope.

When suggesting queries, format them as:
- Query type (email/username/domain/ip_address/phone/name)
- Query value
- Brief rationale for why this search would advance the investigation
"""

SUMMARIZATION_SYSTEM_PROMPT = """You are a report generator for the Mary Poppins Digital Intelligence Platform.

Generate concise, professional investigation summaries suitable for:
- Case file documentation
- Lead investigator briefings
- Inter-agency intelligence sharing

Format: structured sections with clear headings and bullet points.
"""


# ---------------------------------------------------------------------------
# OSINT Agent Service
# ---------------------------------------------------------------------------

class OsintAgentService:
    """LLM-powered OSINT investigation assistant.

    This service provides conversational AI capabilities for OSINT
    investigations, leveraging the platform's LLM service for
    provider-agnostic model access.
    """

    def __init__(self, llm_service: Any):
        """Initialize with a reference to the LLM service.

        Args:
            llm_service: Instance of LLMService from services/llm/service.py
        """
        self.llm_service = llm_service

    async def chat(
        self,
        messages: list[AgentMessage],
        context: Optional[AgentContext] = None,
    ) -> AgentMessage:
        """Process a chat message and return an agent response.

        Args:
            messages: Conversation history.
            context: Current investigation context.

        Returns:
            Agent response with optional suggested queries.
        """
        start = time.monotonic()
        logger.info(
            "Agent chat: messages=%d context=%s",
            len(messages), bool(context),
        )

        # Build context-aware system prompt
        system_prompt = OSINT_AGENT_SYSTEM_PROMPT
        if context:
            system_prompt += f"\n\nCURRENT CONTEXT:\n"
            if context.current_query_type and context.current_query_value:
                system_prompt += f"- Active query: {context.current_query_type} = {context.current_query_value}\n"
            if context.case_id:
                system_prompt += f"- Linked case: {context.case_id}"
                if context.case_title:
                    system_prompt += f" ({context.case_title})"
                system_prompt += "\n"
            if context.findings_summary:
                system_prompt += f"- {len(context.findings_summary)} findings available\n"
                for f in context.findings_summary[:10]:
                    system_prompt += f"  [{f.get('type', '?')}] {f.get('summary', 'N/A')} (conf: {f.get('confidence', 0):.0%})\n"

        # Convert to LLM request format
        from .service import LLMRequest, LLMTaskType  # noqa: avoid circular at module level

        llm_messages = []
        for msg in messages:
            if msg.role != AgentRole.SYSTEM:
                llm_messages.append({
                    "role": msg.role.value,
                    "content": msg.content,
                })

        try:
            request = LLMRequest(
                task=LLMTaskType.OSINT_AGENT,
                system_prompt=system_prompt,
                messages=llm_messages,
                temperature=0.7,
                max_tokens=2048,
            )
            response = await self.llm_service.complete(request)

            elapsed = int((time.monotonic() - start) * 1000)
            logger.info("Agent response: elapsed=%dms", elapsed)

            # Parse suggested queries from response (in production: structured output)
            suggested_queries = self._extract_suggested_queries(response.content)

            return AgentMessage(
                role=AgentRole.ASSISTANT,
                content=response.content,
                suggested_queries=suggested_queries,
                metadata={
                    "provider": response.provider.value,
                    "model": response.model,
                    "elapsed_ms": elapsed,
                },
            )

        except Exception as exc:
            logger.error("Agent chat failed: %s", exc)
            return AgentMessage(
                role=AgentRole.ASSISTANT,
                content=f"I encountered an error processing your request: {exc}. Please try again.",
                metadata={"error": str(exc)},
            )

    async def suggest_queries(
        self,
        context: AgentContext,
    ) -> list[SuggestedQuery]:
        """Generate search query suggestions based on current findings."""
        if not context.findings_summary:
            return []

        prompt = (
            "Based on the current investigation findings, suggest 3-5 follow-up "
            "OSINT searches that would advance the investigation. For each, provide:\n"
            "- query_type (email/username/domain/ip_address/phone/name)\n"
            "- query_value (the actual search term)\n"
            "- rationale (why this search is valuable)\n"
            "- priority (high/medium/low)\n"
        )

        message = AgentMessage(
            role=AgentRole.USER,
            content=prompt,
        )

        response = await self.chat([message], context)
        return response.suggested_queries

    async def analyze_patterns(
        self,
        findings: list[dict[str, Any]],
    ) -> str:
        """Analyze patterns across OSINT findings."""
        prompt = (
            "Analyze the following OSINT findings for patterns, correlations, "
            "and anomalies. Identify:\n"
            "1. Cross-platform connections\n"
            "2. Timeline patterns\n"
            "3. Risk indicators\n"
            "4. Gaps in the investigation\n\n"
            "Findings:\n"
        )
        for f in findings[:20]:
            prompt += f"  - [{f.get('type', '?')}] {f.get('summary', 'N/A')}\n"

        message = AgentMessage(role=AgentRole.USER, content=prompt)
        response = await self.chat([message])
        return response.content

    async def summarize_investigation(
        self,
        context: AgentContext,
        conversation_history: list[AgentMessage],
    ) -> InvestigationSummary:
        """Generate a structured investigation summary."""
        prompt = (
            "Generate a comprehensive investigation summary based on the "
            "conversation history and findings. Include:\n"
            "- Title\n"
            "- Key findings (bullet points)\n"
            "- Risk assessment (critical/high/medium/low)\n"
            "- Recommended actions\n"
            "- Entities of interest\n"
        )

        messages = conversation_history + [
            AgentMessage(role=AgentRole.USER, content=prompt),
        ]

        response = await self.chat(messages, context)

        # In production: parse structured output from LLM
        return InvestigationSummary(
            title=f"Investigation Summary — {context.current_query_value or 'Unknown'}",
            key_findings=["Finding parsing from LLM response"],
            risk_assessment="medium",
            recommended_actions=["Continue monitoring", "Cross-reference with cases"],
            entities_of_interest=[],
        )

    @staticmethod
    def _extract_suggested_queries(content: str) -> list[SuggestedQuery]:
        """Extract suggested queries from agent response text.

        In production, use structured output (JSON mode) for reliable parsing.
        """
        # Placeholder — in production, parse from structured LLM output
        return []
